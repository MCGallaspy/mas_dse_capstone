{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "\n",
    "This notebook explores classifers for fire occurrence using pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell locally to set up Spark. On AWS EMR the \"spark\" context is provided already\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"\"\"\\\n",
    "    --conf spark.driver.bindAddress=localhost \\\n",
    "    --conf spark.driver.host=localhost \\\n",
    "    --packages \"org.apache.hadoop:hadoop-aws:2.7.3\" pyspark-shell\"\"\"\n",
    "os.environ['SPARK_MASTER_HOST'] = 'localhost'\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "conf = SparkConf()\\\n",
    "    .setMaster(\"local[12]\")\\\n",
    "    .setAppName(\"fire-occurrence-classifer\")\\\n",
    "    .set(\"spark.executor.memory\", \"14G\") \\\n",
    "    .set(\"spark.driver.memory\", \"14G\") \\\n",
    "    .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 date  precipitation_amount_mm  relative_humidity_%  \\\n",
      "0  946684800000000000                      0.0                 40.5   \n",
      "\n",
      "   specific_humidity_kg/kg  surface_downwelling_shortwave_flux_in_air_W_m-2  \\\n",
      "0                    0.006                                            139.7   \n",
      "\n",
      "   wind_from_direction_Degrees_Clockwise_from_north  wind_speed_m/s  \\\n",
      "0                                             222.0             2.1   \n",
      "\n",
      "   max_air_temperature_K  min_air_temperature_K  burning_index_g_Unitless  \\\n",
      "0                  292.0                  282.2                      31.0   \n",
      "\n",
      "   dead_fuel_moisture_100hr_Percent  dead_fuel_moisture_1000hr_Percent  \\\n",
      "0                              12.3                               12.1   \n",
      "\n",
      "   energy_release_component-g_Unitless  potential_evapotranspiration_mm  \\\n",
      "0                                 48.0                              1.8   \n",
      "\n",
      "   mean_vapor_pressure_deficit_kPa  fire_occurred  acres_burned fire_name  \\\n",
      "0                             0.69              0           NaN      None   \n",
      "\n",
      "   longitude   latitude  \n",
      "0   -117.975  33.566667  \n"
     ]
    }
   ],
   "source": [
    "# Must rename a column that is invalid for pyspark\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(\"integratedData.parquet.gz\")\n",
    "cols = [col for col in df]\n",
    "renamed = [col.replace(\" \", \"_\") for col in cols]\n",
    "df.date = pd.to_numeric(df.date)\n",
    "df.fire_occurred = df.fire_occurred.astype(int)\n",
    "df = df.rename(columns=dict(zip(cols, renamed)))\n",
    "print(df.head(1))\n",
    "df.to_parquet(\"integratedData.renamed.parquet.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date=978480000000000000, precipitation_amount_mm=0.0, relative_humidity_%=9.9, specific_humidity_kg/kg=0.0019100000000000002, surface_downwelling_shortwave_flux_in_air_W_m-2=147.20000000000002, wind_from_direction_Degrees_Clockwise_from_north=37.0, wind_speed_m/s=4.4, max_air_temperature_K=297.0, min_air_temperature_K=281.2, burning_index_g_Unitless=68.0, dead_fuel_moisture_100hr_Percent=6.9, dead_fuel_moisture_1000hr_Percent=9.5, energy_release_component-g_Unitless=72.0, potential_evapotranspiration_mm=4.800000000000001, mean_vapor_pressure_deficit_kPa=1.73, fire_occurred=1, acres_burned=10438.01953125, fire_name='VIEJAS', longitude=-116.76666663333334, latitude=32.81666666666667, __index_level_0__=513149),\n",
       " Row(date=978480000000000000, precipitation_amount_mm=0.0, relative_humidity_%=9.8, specific_humidity_kg/kg=0.0019500000000000001, surface_downwelling_shortwave_flux_in_air_W_m-2=145.1, wind_from_direction_Degrees_Clockwise_from_north=54.0, wind_speed_m/s=4.9, max_air_temperature_K=295.2, min_air_temperature_K=280.8, burning_index_g_Unitless=76.0, dead_fuel_moisture_100hr_Percent=6.300000000000001, dead_fuel_moisture_1000hr_Percent=8.6, energy_release_component-g_Unitless=77.0, potential_evapotranspiration_mm=4.800000000000001, mean_vapor_pressure_deficit_kPa=1.56, fire_occurred=1, acres_burned=10438.01953125, fire_name='VIEJAS', longitude=-116.72499996666667, latitude=32.81666666666667, __index_level_0__=513150)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"integratedData.renamed.parquet.gz\")\n",
    "# df.printSchema()\n",
    "# df = df.withColumnRenamed(\"surface_downwelling_shortwave_flux_in_air_W m-2\",\n",
    "#                           \"surface_downwelling_shortwave_flux_in_air_W_m-2\")\n",
    "# df.printSchema()\n",
    "spark.registerDataFrameAsTable(df, \"integratedData\")\n",
    "spark.sql(\"SELECT * FROM integratedData WHERE fire_occurred=1 LIMIT 2\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|fire_occurred|  count|\n",
      "+-------------+-------+\n",
      "|            1|   1743|\n",
      "|            0|6499707|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"fire_occurred\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "cols = [\n",
    "    'date',\n",
    "    'precipitation_amount_mm', \n",
    "    'relative_humidity_%',\n",
    "    'specific_humidity_kg/kg',\n",
    "    'surface_downwelling_shortwave_flux_in_air_W_m-2',\n",
    "    'wind_from_direction_Degrees_Clockwise_from_north',\n",
    "    'wind_speed_m/s',\n",
    "    'max_air_temperature_K',\n",
    "    'min_air_temperature_K',\n",
    "    'burning_index_g_Unitless',\n",
    "    'dead_fuel_moisture_100hr_Percent',\n",
    "    'dead_fuel_moisture_1000hr_Percent',\n",
    "    'energy_release_component-g_Unitless',\n",
    "    'potential_evapotranspiration_mm', \n",
    "    'mean_vapor_pressure_deficit_kPa',\n",
    "#     'fire_occurred',\n",
    "#     'acres_burned',\n",
    "#     'fire_name',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=cols,\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(df)\n",
    "rf = RandomForestClassifier(\n",
    "    numTrees=20,\n",
    "    maxDepth=8,\n",
    "    labelCol=\"fire_occurred\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseVector(17, {0: 0.1743, 1: 0.0047, 2: 0.0557, 3: 0.0479, 4: 0.0822, 5: 0.0424, 6: 0.0725, 7: 0.051, 8: 0.0549, 9: 0.0145, 10: 0.0528, 11: 0.058, 12: 0.0393, 13: 0.0561, 14: 0.0532, 15: 0.0867, 16: 0.0539})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = rf.fit(output)\n",
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1742823 , 0.00466235, 0.05568691, 0.04789484, 0.08224204,\n",
       "       0.04237697, 0.07246099, 0.05098685, 0.05488177, 0.01450643,\n",
       "       0.0528353 , 0.05801185, 0.03932865, 0.05612418, 0.05318839,\n",
       "       0.08667671, 0.05385347])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureImportances.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-------+\n",
      "|fire_occurred|prediction|  count|\n",
      "+-------------+----------+-------+\n",
      "|            1|       0.0|   1743|\n",
      "|            0|       0.0|6499707|\n",
      "+-------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(output).groupBy(\"fire_occurred\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "cols = [\n",
    "    'date',\n",
    "    'precipitation_amount_mm', \n",
    "    'relative_humidity_%',\n",
    "    'specific_humidity_kg/kg',\n",
    "    'surface_downwelling_shortwave_flux_in_air_W_m-2',\n",
    "    'wind_from_direction_Degrees_Clockwise_from_north',\n",
    "    'wind_speed_m/s',\n",
    "    'max_air_temperature_K',\n",
    "    'min_air_temperature_K',\n",
    "    'burning_index_g_Unitless',\n",
    "    'dead_fuel_moisture_100hr_Percent',\n",
    "    'dead_fuel_moisture_1000hr_Percent',\n",
    "    'energy_release_component-g_Unitless',\n",
    "    'potential_evapotranspiration_mm', \n",
    "    'mean_vapor_pressure_deficit_kPa',\n",
    "#     'fire_occurred',\n",
    "#     'acres_burned',\n",
    "#     'fire_name',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=cols,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    numTrees=20,\n",
    "    maxDepth=8,\n",
    "    labelCol=\"fire_occurred\",\n",
    "    seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "# paramGrid = ParamGridBuilder() \\\n",
    "#     .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "#     .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "#     .build()\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='prediction', \n",
    "    labelCol='fire_occurred',\n",
    "    metricName='areaUnderPR')\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)  # use 3+ folds in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cvModel = crossval.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|fire_occurred|prediction|\n",
      "+-------------+----------+\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvModel.transform(df).select(\"fire_occurred\", \"prediction\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-------+\n",
      "|fire_occurred|prediction|  count|\n",
      "+-------------+----------+-------+\n",
      "|            1|       0.0|   1743|\n",
      "|            0|       0.0|6499707|\n",
      "+-------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvModel.transform(df).groupBy(\"fire_occurred\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "cols = [\n",
    "    'date',\n",
    "    'precipitation_amount_mm', \n",
    "    'relative_humidity_%',\n",
    "    'specific_humidity_kg/kg',\n",
    "    'surface_downwelling_shortwave_flux_in_air_W_m-2',\n",
    "    'wind_from_direction_Degrees_Clockwise_from_north',\n",
    "    'wind_speed_m/s',\n",
    "    'max_air_temperature_K',\n",
    "    'min_air_temperature_K',\n",
    "    'burning_index_g_Unitless',\n",
    "    'dead_fuel_moisture_100hr_Percent',\n",
    "    'dead_fuel_moisture_1000hr_Percent',\n",
    "    'energy_release_component-g_Unitless',\n",
    "    'potential_evapotranspiration_mm', \n",
    "    'mean_vapor_pressure_deficit_kPa',\n",
    "#     'fire_occurred',\n",
    "#     'acres_burned',\n",
    "#     'fire_name',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=cols,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    maxIter=10,\n",
    "    labelCol=\"fire_occurred\",\n",
    "    seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "# paramGrid = ParamGridBuilder() \\\n",
    "#     .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "#     .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "#     .build()\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='prediction', \n",
    "    labelCol='fire_occurred',\n",
    "    metricName='areaUnderPR')\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)  # use 3+ folds in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cvModel = crossval.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-------+\n",
      "|fire_occurred|prediction|  count|\n",
      "+-------------+----------+-------+\n",
      "|            1|       0.0|   1742|\n",
      "|            0|       0.0|6499707|\n",
      "|            1|       1.0|      1|\n",
      "+-------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvModel.transform(df).groupBy(\"fire_occurred\", \"prediction\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
