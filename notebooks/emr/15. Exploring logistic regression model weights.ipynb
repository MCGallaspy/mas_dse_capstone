{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1590806483524_0002</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-20-104.ec2.internal:20888/proxy/application_1590806483524_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-23-112.ec2.internal:8042/node/containerlogs/container_1590806483524_0002_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "s3_url = 's3a://dse-cohort5-group5/wildfire_capstone/integratedData.pca.parquet.gz'\n",
    "pca_df = spark.read.parquet(s3_url)\n",
    "pca_df.createOrReplaceTempView('pca')\n",
    "\n",
    "base_df = spark.read.parquet('s3a://dse-cohort5-group5/wildfire_capstone/integratedData.renamed.parquet.gz')\n",
    "base_df.createOrReplaceTempView(\"fire_occurrences\")\n",
    "\n",
    "join_query = \"\"\"\n",
    "SELECT fire_occurrences.date,          fire_occurrences.latitude,     fire_occurrences.longitude,\n",
    "       fire_occurrences.fire_occurred, fire_occurrences.acres_burned, pca.pcaFeatures,\n",
    "       fire_occurrences.fire_name,\n",
    "       from_unixtime(cast(fire_occurrences.date/1e9 as long), 'yyyy') as year,\n",
    "       from_unixtime(cast(fire_occurrences.date/1e9 as long), 'MM') as month,\n",
    "       from_unixtime(cast(fire_occurrences.date/1e9 as long), 'dd') as day\n",
    "FROM fire_occurrences, pca\n",
    "WHERE pca.date      = fire_occurrences.date\n",
    "  AND pca.latitude  = fire_occurrences.latitude\n",
    "  AND pca.longitude = fire_occurrences.longitude\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "N_FEATURES_TO_KEEP = 40\n",
    "\n",
    "def to_array(col):\n",
    "    def to_array_(v):\n",
    "        return v.toArray().tolist()\n",
    "    # Important: asNondeterministic requires Spark 2.3 or later\n",
    "    # It can be safely removed i.e.\n",
    "    # return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "    # but at the cost of decreased performance\n",
    "    return udf(to_array_, ArrayType(DoubleType())).asNondeterministic()(col)\n",
    "\n",
    "joined_df = spark.sql(join_query)\n",
    "joined_df = joined_df.withColumn(\"pcaFeaturesArr\", to_array(col(\"pcaFeatures\")))\\\n",
    "                     .select([\"fire_occurrences.date\", \"fire_occurrences.latitude\", \"fire_occurrences.longitude\",\n",
    "                              \"fire_occurrences.fire_occurred\", \"fire_occurrences.acres_burned\",\n",
    "                              \"year\", \"month\", \"day\", \"fire_occurrences.fire_name\"]\n",
    "                             + [col(\"pcaFeaturesArr\")[i] for i in range(N_FEATURES_TO_KEEP)])\n",
    "\n",
    "joined_df.cache()\n",
    "joined_df.createOrReplaceTempView(\"joined\")\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def train_logistic_regression_and_save_model(\n",
    "    fires_weight=10000, # Chosen for good recall (About 80%) on validation set with 0.5 threshold.\n",
    "    no_fires_weight=1,\n",
    "    predict_year=2019):\n",
    "    \n",
    "    # Add class weights for the Logistic Regression classifier below\n",
    "        # Add class weights for the Logistic Regression classifier below\n",
    "    with_fires_df = spark.sql(\"\"\"\n",
    "    SELECT *, {} as weight FROM joined WHERE joined.fire_occurred = 1\n",
    "    \"\"\".format(fires_weight))\n",
    "    without_fires_df = spark.sql(\"\"\"\n",
    "    SELECT *, {} as weight FROM joined WHERE joined.fire_occurred = 0\n",
    "    \"\"\".format(no_fires_weight))\n",
    "\n",
    "    with_fires_train    = with_fires_df.filter(\"year != {}\".format(predict_year))\n",
    "    with_fires_test     = with_fires_df.filter(\"year  = {}\".format(predict_year))\n",
    "    without_fires_train = without_fires_df.filter(\"year != {}\".format(predict_year))\n",
    "    without_fires_test  = without_fires_df.filter(\"year  = {}\".format(predict_year))\n",
    "    \n",
    "    train_df = with_fires_train.union(without_fires_train)\n",
    "    test_df  = with_fires_test.union(without_fires_test)\n",
    "    train_df.cache()\n",
    "    test_df.cache()\n",
    "    print(\"Train count, test count:\", train_df.count(), test_df.count())\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"pcaFeaturesArr[{}]\".format(i) for i in range(0, 40)],\n",
    "        outputCol=\"features\")\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol='features',\n",
    "        probabilityCol='probability',\n",
    "        labelCol='fire_occurred',\n",
    "        weightCol='weight',\n",
    "        family=\"binomial\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    model_s3_url = \"s3://dse-cohort5-group5/wildfire_capstone/logistic_regression_predictions/models/\"\\\n",
    "             \"year={year}/weight={weight}/model.sparkobject\"\\\n",
    "             .format(year=predict_year, weight=fires_weight)\n",
    "    print(\"saving model to\", model_s3_url)\n",
    "    model.save(model_s3_url)\n",
    "    \n",
    "    print(\"*\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count, test count: 6500560 324850\n",
      "saving model to s3://dse-cohort5-group5/wildfire_capstone/logistic_regression_predictions/models/year=2007/weight=10000/model.sparkobject\n",
      "********************************************************************************"
     ]
    }
   ],
   "source": [
    "train_logistic_regression_and_save_model(fires_weight=10000, predict_year=2007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count, test count: 6500560 324850\n",
      "saving model to s3://dse-cohort5-group5/wildfire_capstone/logistic_regression_predictions/models/year=2010/weight=10000/model.sparkobject\n",
      "********************************************************************************"
     ]
    }
   ],
   "source": [
    "train_logistic_regression_and_save_model(fires_weight=10000, predict_year=2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for year 2010\n",
      "Coefficients: [0.08881099235164998,0.13632301177193015,0.11020134395069991,0.0003229175303864513,0.06504448428938507,-0.30694896684238937,-0.1613960726706734,0.11372871376090295,0.38890873919913177,0.2634028293911873,-0.17968386306572232,-0.23232996389199795,-0.22480312207042655,-0.21059037746396092,0.22663742494342698,-0.07903999097628758,-0.5092802827324341,-0.05380340971227158,-0.27770262442585214,0.2260845067438809,0.0529373860383622,-0.005954282211155398,-0.10153784772257698,0.1920697089745939,0.16544992364723882,-0.002088028040021257,-0.027605722592341096,-0.1639381703633287,0.399023898350511,-0.1660266922838218,-0.08063762572915308,0.012606607927062531,-0.20326068628548385,0.11967534081476812,0.27269747774308745,-0.00862799542542835,0.0062055509264404025,0.1860452797761952,-0.20145328956293393,-0.12190226857159799]\n",
      "Intercept: -2.300353214326727"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "year = 2010\n",
    "pipeline_model = PipelineModel.load(\n",
    "    's3://dse-cohort5-group5/wildfire_capstone/logistic_regression_predictions/models/'\n",
    "    'year={}/weight=10000/model.sparkobject'.format(year))\n",
    "print(\"Coefficients for year {}\".format(year))\n",
    "print(\"Coefficients:\", pipeline_model.stages[-1].coefficients)\n",
    "print(\"Intercept:\", pipeline_model.stages[-1].intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for year 2007\n",
      "Coefficients: [0.0751749927532972,0.11931195800526559,0.11870575796818895,0.03175585137308058,0.06355862698396718,-0.26557461018751777,-0.09921473261382105,0.0788782349877773,0.28265808743749854,0.21409965783457982,-0.21541297743912155,-0.15445135855073544,-0.2206521720803125,-0.23769853504149094,0.12532247029072785,-0.09515398917180073,-0.41648255172840287,-0.07249897205843178,-0.1698772074989707,0.13681748574973437,-0.05007667848693983,0.09347101189083838,0.012020792785904785,0.16518160717803462,0.2128146979168255,-0.08861551642551775,-0.0944462656740906,-0.1526228187362223,0.3310517170743802,-0.1688107702386254,-0.07990266124895551,0.021978865991899185,-0.21819743729719543,0.0911705688664554,0.33186866877577725,-0.016854531432428706,-0.16668388215972868,0.08715283470992505,-0.27109544502467553,-0.1802844294983804]\n",
      "Intercept: -2.0240806207467372"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "year = 2007\n",
    "pipeline_model = PipelineModel.load(\n",
    "    's3://dse-cohort5-group5/wildfire_capstone/logistic_regression_predictions/models/'\n",
    "    'year={}/weight=10000/model.sparkobject'.format(year))\n",
    "print(\"Coefficients for year {}\".format(year))\n",
    "print(\"Coefficients:\", pipeline_model.stages[-1].coefficients)\n",
    "print(\"Intercept:\", pipeline_model.stages[-1].intercept)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
