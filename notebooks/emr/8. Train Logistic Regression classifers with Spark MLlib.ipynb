{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1587935937072_0003</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-28-68.ec2.internal:20888/proxy/application_1587935937072_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-24-227.ec2.internal:8042/node/containerlogs/container_1587935937072_0003_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "6825410"
     ]
    }
   ],
   "source": [
    "s3_url = 's3a://dse-cohort5-group5/wildfire_capstone/integratedData.pca.parquet.gz'\n",
    "pca_df = spark.read.parquet(s3_url)\n",
    "pca_df.createOrReplaceTempView('pca')\n",
    "pca_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6826300"
     ]
    }
   ],
   "source": [
    "base_df = spark.read.parquet('s3a://dse-cohort5-group5/wildfire_capstone/integratedData.renamed.parquet.gz')\n",
    "base_df.createOrReplaceTempView(\"fire_occurrences\")\n",
    "base_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_query = \"\"\"\n",
    "SELECT fire_occurrences.date,          fire_occurrences.latitude,     fire_occurrences.longitude,\n",
    "       fire_occurrences.fire_occurred, fire_occurrences.acres_burned, pca.pcaFeatures,\n",
    "       from_unixtime(cast(fire_occurrences.date/1e9 as long), 'yyyy') as year,\n",
    "       from_unixtime(cast(fire_occurrences.date/1e9 as long), 'MM') as month,\n",
    "       from_unixtime(cast(fire_occurrences.date/1e9 as long), 'dd') as day\n",
    "FROM fire_occurrences, pca\n",
    "WHERE pca.date      = fire_occurrences.date\n",
    "  AND pca.latitude  = fire_occurrences.latitude\n",
    "  AND pca.longitude = fire_occurrences.longitude\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "N_FEATURES_TO_KEEP = 40\n",
    "\n",
    "def to_array(col):\n",
    "    def to_array_(v):\n",
    "        return v.toArray().tolist()\n",
    "    # Important: asNondeterministic requires Spark 2.3 or later\n",
    "    # It can be safely removed i.e.\n",
    "    # return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "    # but at the cost of decreased performance\n",
    "    return udf(to_array_, ArrayType(DoubleType())).asNondeterministic()(col)\n",
    "\n",
    "joined_df = spark.sql(join_query)\n",
    "joined_df = joined_df.withColumn(\"pcaFeaturesArr\", to_array(col(\"pcaFeatures\")))\\\n",
    "                     .select([\"fire_occurrences.date\", \"fire_occurrences.latitude\", \"fire_occurrences.longitude\",\n",
    "                              \"fire_occurrences.fire_occurred\", \"fire_occurrences.acres_burned\",\n",
    "                              \"year\", \"month\", \"day\"]\n",
    "                             + [col(\"pcaFeaturesArr\")[i] for i in range(N_FEATURES_TO_KEEP)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6825410"
     ]
    }
   ],
   "source": [
    "joined_df.cache()\n",
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.createOrReplaceTempView(\"joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- fire_occurred: integer (nullable = true)\n",
      " |-- acres_burned: double (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- pcaFeaturesArr[0]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[1]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[2]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[3]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[4]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[5]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[6]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[7]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[8]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[9]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[10]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[11]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[12]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[13]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[14]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[15]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[16]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[17]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[18]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[19]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[20]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[21]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[22]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[23]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[24]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[25]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[26]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[27]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[28]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[29]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[30]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[31]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[32]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[33]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[34]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[35]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[36]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[37]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[38]: double (nullable = true)\n",
      " |-- pcaFeaturesArr[39]: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "joined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def evaluate_weighted_logistic_regression(fires_weight=1, no_fires_weight=1):\n",
    "    # Add class weights for the Logistic Regression classifier below\n",
    "    with_fires_df = spark.sql(\"\"\"\n",
    "    SELECT *, {} as weight FROM joined WHERE joined.fire_occurred = 1\n",
    "    \"\"\".format(fires_weight))\n",
    "    without_fires_df = spark.sql(\"\"\"\n",
    "    SELECT *, {} as weight FROM joined WHERE joined.fire_occurred = 0\n",
    "    \"\"\".format(no_fires_weight))\n",
    "\n",
    "    with_fires_train,    with_fires_test    = with_fires_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    without_fires_train, without_fires_test = without_fires_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    train_df = with_fires_train.union(without_fires_train).sample(fraction=1.0, seed=42)\n",
    "    test_df  = with_fires_test.union(without_fires_test).sample(fraction=1.0, seed=42)\n",
    "    train_df.cache()\n",
    "    test_df.cache()\n",
    "    print(\"Train count, test count:\", train_df.count(), test_df.count())\n",
    "\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"pcaFeaturesArr[{}]\".format(i) for i in range(0, 40)],\n",
    "        outputCol=\"features\")\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol='features', \n",
    "        labelCol='fire_occurred',\n",
    "        weightCol='weight',\n",
    "        family=\"binomial\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    predictions = model.transform(test_df)\n",
    "    predictions.createOrReplaceTempView('predictions')\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"fire_occurred\", predictionCol=\"prediction\",\n",
    "                                                  metricName=\"f1\")\n",
    "\n",
    "    print(\"Test results for fire/no fire weights\", fires_weight, no_fires_weight)\n",
    "\n",
    "    f1 = evaluator.evaluate(predictions)\n",
    "    print(\"Test set f1 score = \" + str(f1))\n",
    "\n",
    "    true_positive = spark.sql(\"\"\"\n",
    "    SELECT * FROM predictions WHERE fire_occurred = 1 AND  prediction = 1\"\"\")\n",
    "    true_positive = true_positive.count()\n",
    "\n",
    "    false_negative = spark.sql(\"\"\"\n",
    "    SELECT * FROM predictions WHERE fire_occurred = 1 AND  prediction = 0\"\"\")\n",
    "    false_negative = false_negative.count()\n",
    "\n",
    "    true_negative = spark.sql(\"\"\"\n",
    "    SELECT * FROM predictions WHERE fire_occurred = 0 AND  prediction = 0\"\"\")\n",
    "    true_negative = true_negative.count()\n",
    "\n",
    "    false_positive = spark.sql(\"\"\"\n",
    "    SELECT * FROM predictions WHERE fire_occurred = 0 AND  prediction = 1\"\"\")\n",
    "    false_positive = false_positive.count()\n",
    "    \n",
    "    print(\"TP:\", true_positive)\n",
    "    print(\"FP:\", false_positive)\n",
    "    print(\"TN:\", true_negative)\n",
    "    print(\"FN:\", false_negative)\n",
    "    \n",
    "    print(\"% of fires recalled: {:%}\".format(true_positive/(true_positive + false_negative)))\n",
    "    print(\"Accuracy for non-fires: {:%}\".format(true_negative/(true_negative + false_positive)))\n",
    "    \n",
    "    print(\"*\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count, test count: 5461464 1363946\n",
      "Test results for fire/no fire weights 1 1\n",
      "Test set f1 score = 0.9996621919227574\n",
      "TP: 46\n",
      "FP: 24\n",
      "TN: 1363556\n",
      "FN: 320\n",
      "% of fires recalled: 12.568306%\n",
      "Accuracy for non-fires: 99.998240%\n",
      "********************************************************************************"
     ]
    }
   ],
   "source": [
    "evaluate_weighted_logistic_regression(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count, test count: 5461464 1363946\n",
      "Test results for fire/no fire weights 1000.0 1\n",
      "Test set f1 score = 0.9838473118140173\n",
      "TP: 308\n",
      "FP: 42607\n",
      "TN: 1320973\n",
      "FN: 58\n",
      "% of fires recalled: 84.153005%\n",
      "Accuracy for non-fires: 96.875358%\n",
      "********************************************************************************"
     ]
    }
   ],
   "source": [
    "evaluate_weighted_logistic_regression(1e3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count, test count: 5461464 1363946\n",
      "Test results for fire/no fire weights 10000.0 1\n",
      "Test set f1 score = 0.9194332456547004\n",
      "TP: 352\n",
      "FP: 202750\n",
      "TN: 1160830\n",
      "FN: 14\n",
      "% of fires recalled: 96.174863%\n",
      "Accuracy for non-fires: 85.131052%\n",
      "********************************************************************************"
     ]
    }
   ],
   "source": [
    "evaluate_weighted_logistic_regression(1e4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count, test count: 5461464 1363946\n",
      "Test results for fire/no fire weights 100000.0 1\n",
      "Test set f1 score = 0.8250901631433544\n",
      "TP: 366\n",
      "FP: 405558\n",
      "TN: 958022\n",
      "FN: 0\n",
      "% of fires recalled: 100.000000%\n",
      "Accuracy for non-fires: 70.257851%\n",
      "********************************************************************************"
     ]
    }
   ],
   "source": [
    "evaluate_weighted_logistic_regression(1e5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count, test count: 5461464 1363946\n",
      "Test results for fire/no fire weights 1000000.0 1\n",
      "Test set f1 score = 0.7718372685731948\n",
      "TP: 366\n",
      "FP: 506266\n",
      "TN: 857314\n",
      "FN: 0\n",
      "% of fires recalled: 100.000000%\n",
      "Accuracy for non-fires: 62.872292%\n",
      "********************************************************************************"
     ]
    }
   ],
   "source": [
    "evaluate_weighted_logistic_regression(1e6, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
